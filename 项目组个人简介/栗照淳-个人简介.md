# 栗照淳-个人简介

![微信图片_20260208095205_39_173.jpeg](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/Pd6l2Z7748m9Ml7M/img/41fff158-900b-4a14-8b99-0de42c610440.jpeg)

**栗照淳**

AI基础与理论

RL算法及其泛化性研究

教育背景：

2020-2024 北京理工大学 自动化 学士学位  
2024-2029（预计）北京理工大学 控制科学与控制工程 博士  
2024-2029（预计）北京中关村学院  大模型应用驱动下的大规模强化学习算法研究与实践项目组 博士

研究方向：

专注于AI基础理论研究。重点关注大模型时代下，不同算法、不同范式的泛化性（ID、OOD）分析，以及LLM后训练算法的不足与改进。

研究成果：

1.  【ICML submission】Distribution-Centric Policy Optimization Dominates Exploration-Exploitation Trade-off
    
    *   **Zhaochun Li**\*, Chen Wang**\***, Bai Jionghao\*, Guanting Dong, Shisheng Cui, Ge Lan, Zhou Zhao, Yue Wang
        
    *   证明了在LLM强化学习后训练中，相较于单个样本，由样本总体所形成的期望梯度占主导地位，并依此提出了完全on-policy的熵控制算法DCPO。
        
2.  【ICML submission】Towards a Theoretical Understanding to the Generalization of RLHF
    
    *   **Zhaochun Li**\*, Mingyang Yi, Yue Wang, Shisheng Cui, Yong Liu
        
    *   首次建立RLHF端到端泛化性分析的分析框架，并证明RLHF算法的泛化性误差与样本量之间的关系，分析得到泛化性误差的阶为$\tilde{\mathcal{O}}(\frac{1}{\sqrt{n}})$。